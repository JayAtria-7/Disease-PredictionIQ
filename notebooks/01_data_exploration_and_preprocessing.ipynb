{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f8b9b8",
   "metadata": {},
   "source": [
    "# Heart Disease Detection Capstone Project\n",
    "## Phase 1: Data Exploration and Preprocessing\n",
    "## Phase 2: Baseline Model Development\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for predicting heart disease using multiple classification algorithms with proper evaluation metrics suitable for healthcare applications.\n",
    "\n",
    "### Project Overview\n",
    "- **Objective**: Predict the presence of heart disease based on diagnostic test results\n",
    "- **Dataset**: Clinical diagnostic data with 14 features and 1 target variable\n",
    "- **Models**: Decision Tree, Random Forest, Logistic Regression, Support Vector Machine\n",
    "- **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- **Optimization**: GridSearchCV with 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307595c4",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             roc_curve, auc, classification_report)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794577e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../heart_disease_dataset.csv')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf85510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMissing Values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nDuplicate Rows: {df.duplicated().sum()}\")\n",
    "print(f\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "print(f\"\\nData Quality Summary:\")\n",
    "print(f\"  • Total cells: {df.shape[0] * df.shape[1]}\")\n",
    "print(f\"  • Missing cells: {df.isnull().sum().sum()}\")\n",
    "print(f\"  • Data completeness: 100.0%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ffbde",
   "metadata": {},
   "source": [
    "## Section 2: Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTarget Variable Distribution:\")\n",
    "print(df['heart_disease'].value_counts())\n",
    "print(f\"\\nTarget Variable Proportions:\")\n",
    "print(df['heart_disease'].value_counts(normalize=True))\n",
    "\n",
    "# Create visualizations for target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "counts = df['heart_disease'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].bar(['No Disease (0)', 'Disease (1)'], counts.values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_title('Target Variable Distribution (Count)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "for i, v in enumerate(counts.values):\n",
    "    axes[0].text(i, v + 3, str(v), ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts.values, labels=['No Disease', 'Disease Present'], autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Target Variable Distribution (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/01_target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Target distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "features_to_plot = df.columns[:-1]  # Exclude target\n",
    "n_features = len(features_to_plot)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    axes[idx].hist(df[feature], bins=30, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/02_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature distributions visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = df.corr()\n",
    "target_corr = corr_matrix['heart_disease'].drop('heart_disease').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature Correlations with Target (heart_disease):\")\n",
    "print(target_corr)\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/03_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Correlation heatmap visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vs Target relationships\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE VS TARGET RELATIONSHIPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top features by correlation\n",
    "top_features = target_corr.head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    # Create box plot\n",
    "    data_to_plot = [df[df['heart_disease'] == 0][feature], \n",
    "                    df[df['heart_disease'] == 1][feature]]\n",
    "    bp = axes[idx].boxplot(data_to_plot, labels=['No Disease', 'Disease'], patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('#3498db')\n",
    "    \n",
    "    axes[idx].set_title(f'{feature} vs Heart Disease', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/04_feature_vs_target.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature vs Target visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2a84a",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['heart_disease'])\n",
    "y = df['heart_disease']\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "print(f\"\\nNumber of features: {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(f\"  Class 0 (No Disease): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Disease): {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Preserve class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nData Split (80/20):\")\n",
    "print(f\"  Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Testing set size: {X_test.shape[0]} ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Training set - Class 0: {(y_train == 0).sum()}, Class 1: {(y_train == 1).sum()}\")\n",
    "print(f\"  Testing set - Class 0: {(y_test == 0).sum()}, Class 1: {(y_test == 1).sum()}\")\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "print(\"\\nApplying StandardScaler normalization...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
    "\n",
    "print(\"✓ Data preprocessing completed\")\n",
    "print(\"✓ Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157cbbb",
   "metadata": {},
   "source": [
    "## Section 4: Train Baseline Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store baseline models\n",
    "baseline_models = {}\n",
    "\n",
    "# 1. Decision Tree Classifier\n",
    "print(\"\\n1. Decision Tree Classifier\")\n",
    "print(\"-\" * 60)\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "baseline_models['Decision Tree'] = dt_model\n",
    "print(\"✓ Model trained\")\n",
    "\n",
    "# 2. Random Forest Classifier\n",
    "print(\"\\n2. Random Forest Classifier\")\n",
    "print(\"-\" * 60)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "baseline_models['Random Forest'] = rf_model\n",
    "print(\"✓ Model trained\")\n",
    "\n",
    "# 3. Logistic Regression\n",
    "print(\"\\n3. Logistic Regression\")\n",
    "print(\"-\" * 60)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "baseline_models['Logistic Regression'] = lr_model\n",
    "print(\"✓ Model trained\")\n",
    "\n",
    "# 4. Support Vector Machine\n",
    "print(\"\\n4. Support Vector Machine (RBF kernel)\")\n",
    "print(\"-\" * 60)\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "baseline_models['SVM'] = svm_model\n",
    "print(\"✓ Model trained\")\n",
    "\n",
    "print(\"\\n✓ All baseline models trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🏆 FINAL BEST MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select best model based on multiple criteria\n",
    "print(\"\\n📊 SELECTION CRITERIA:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find best model by different metrics\n",
    "best_by_accuracy = all_models_summary.loc[all_models_summary['Accuracy'].idxmax()]\n",
    "best_by_f1 = all_models_summary.loc[all_models_summary['F1-Score'].idxmax()]\n",
    "best_by_roc = all_models_summary.loc[all_models_summary['ROC-AUC'].idxmax()]\n",
    "best_by_recall = all_models_summary.loc[all_models_summary['Recall'].idxmax()]\n",
    "\n",
    "print(f\"\\n1. Best by Accuracy:  {best_by_accuracy['Model']:30s} ({best_by_accuracy['Accuracy']:.4f})\")\n",
    "print(f\"2. Best by F1-Score:  {best_by_f1['Model']:30s} ({best_by_f1['F1-Score']:.4f})\")\n",
    "print(f\"3. Best by ROC-AUC:   {best_by_roc['Model']:30s} ({best_by_roc['ROC-AUC']:.4f})\")\n",
    "print(f\"4. Best by Recall:    {best_by_recall['Model']:30s} ({best_by_recall['Recall']:.4f})\")\n",
    "\n",
    "# Calculate composite score (weighted average)\n",
    "print(\"\\n📈 COMPOSITE SCORE CALCULATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Weights: Accuracy=25%, Precision=20%, Recall=25%, F1=15%, ROC-AUC=15%\")\n",
    "print(\"(Recall weighted higher for medical diagnosis to minimize false negatives)\")\n",
    "\n",
    "all_models_summary['Composite_Score'] = (\n",
    "    0.25 * all_models_summary['Accuracy'] +\n",
    "    0.20 * all_models_summary['Precision'] +\n",
    "    0.25 * all_models_summary['Recall'] +\n",
    "    0.15 * all_models_summary['F1-Score'] +\n",
    "    0.15 * all_models_summary['ROC-AUC']\n",
    ")\n",
    "\n",
    "# Sort by composite score\n",
    "final_ranking = all_models_summary.sort_values('Composite_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n🏆 FINAL RANKING (Top 10 by Composite Score):\")\n",
    "print(\"=\" * 80)\n",
    "print(final_ranking[['Model', 'Accuracy', 'Recall', 'F1-Score', 'Composite_Score']].head(10).to_string(index=True))\n",
    "\n",
    "# Select the best model\n",
    "best_model_final = final_ranking.iloc[0]\n",
    "best_model_name_final = best_model_final['Model']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ RECOMMENDED MODEL FOR HEART DISEASE DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n🥇 Model: {best_model_name_final}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  ├─ Accuracy:        {best_model_final['Accuracy']:.4f}\")\n",
    "print(f\"  ├─ Precision:       {best_model_final['Precision']:.4f}\")\n",
    "print(f\"  ├─ Recall:          {best_model_final['Recall']:.4f}\")\n",
    "print(f\"  ├─ F1-Score:        {best_model_final['F1-Score']:.4f}\")\n",
    "print(f\"  ├─ ROC-AUC:         {best_model_final['ROC-AUC']:.4f}\")\n",
    "print(f\"  └─ Composite Score: {best_model_final['Composite_Score']:.4f}\")\n",
    "\n",
    "print(\"\\n💡 CLINICAL SIGNIFICANCE:\")\n",
    "print(\"-\" * 80)\n",
    "recall_val = best_model_final['Recall']\n",
    "precision_val = best_model_final['Precision']\n",
    "\n",
    "print(f\"  • Sensitivity (Recall): {recall_val:.1%} of actual disease cases detected\")\n",
    "print(f\"  • Precision: {precision_val:.1%} of positive predictions are correct\")\n",
    "print(f\"  • Balanced performance suitable for medical screening\")\n",
    "\n",
    "if recall_val >= 0.90:\n",
    "    print(\"  ✓ EXCELLENT: Very few missed diagnoses (false negatives)\")\n",
    "elif recall_val >= 0.80:\n",
    "    print(\"  ✓ GOOD: Acceptable rate of missed diagnoses\")\n",
    "else:\n",
    "    print(\"  ⚠ MODERATE: Consider ensemble methods for critical applications\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED FOR DEPLOYMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store the best model for later use\n",
    "final_best_model = all_models[best_model_name_final]\n",
    "print(f\"✓ {best_model_name_final} selected as final model for deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 6: RADAR CHART - TOP 5 MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create radar chart for top 5 models\n",
    "top_5_models = all_models_summary.sort_values('Accuracy', ascending=False).head(5)\n",
    "\n",
    "# Prepare data for radar chart\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "\n",
    "for idx, (_, row) in enumerate(top_5_models.iterrows()):\n",
    "    values = row[categories].tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=9)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_title('Performance Radar Chart - Top 5 Models\\n', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/17_top5_radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Radar chart visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50111518",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 5: COMPREHENSIVE PERFORMANCE HEATMAP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create performance heatmap\n",
    "metrics_df = all_models_summary[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].copy()\n",
    "metrics_df = metrics_df.sort_values('Accuracy', ascending=False)\n",
    "metrics_df_values = metrics_df.set_index('Model')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "sns.heatmap(metrics_df_values, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            center=0.75, vmin=0.5, vmax=1.0, linewidths=0.5, \n",
    "            cbar_kws={'label': 'Score'}, ax=ax)\n",
    "\n",
    "ax.set_title('Performance Heatmap - All Classification Models\\n(Darker Green = Better Performance)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Models', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/16_performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Performance heatmap visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 4: CONFUSION MATRICES - TOP 6 MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top 6 models\n",
    "top_6_results = sorted(all_results, key=lambda x: x['Accuracy'], reverse=True)[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(top_6_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred = result['y_pred']\n",
    "    accuracy = result['Accuracy']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'], annot_kws={'size': 12, 'weight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {accuracy:.4f}', \n",
    "                       fontweight='bold', fontsize=11, pad=10)\n",
    "    axes[idx].set_ylabel('True Label', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted Label', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Top 6 Models', fontsize=15, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/15_top6_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308184f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 3: ROC CURVES - TOP 8 MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top 8 models for ROC curve comparison\n",
    "top_8_results = sorted(all_results, key=lambda x: x['Accuracy'], reverse=True)[:8]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Color palette\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c', '#34495e', '#e67e22']\n",
    "\n",
    "for idx, result in enumerate(top_8_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred_proba = result['y_pred_proba']\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        # Get probabilities for positive class\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            proba = y_pred_proba[:, 1]\n",
    "        else:\n",
    "            proba = y_pred_proba\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        ax.plot(fpr, tpr, label=f'{model_name} (AUC={roc_auc:.3f})',\n",
    "                linewidth=2.5, color=colors[idx % len(colors)])\n",
    "\n",
    "# Plot diagonal line\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Top 8 Classification Models', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=10, framealpha=0.9)\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/14_top8_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ROC curves visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 2: MULTI-METRIC COMPARISON - TOP 10 MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top 10 models by accuracy\n",
    "top_10_models = all_models_summary.sort_values('Accuracy', ascending=False).head(10)\n",
    "\n",
    "# Create multi-metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes.flatten()[idx]\n",
    "    \n",
    "    # Sort by current metric\n",
    "    sorted_data = top_10_models.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Color coding\n",
    "    colors_bar = []\n",
    "    for val in sorted_data[metric].values:\n",
    "        if val == sorted_data[metric].max():\n",
    "            colors_bar.append('#27ae60')  # Green for best\n",
    "        elif val >= sorted_data[metric].quantile(0.75):\n",
    "            colors_bar.append('#f39c12')  # Orange for top quartile\n",
    "        else:\n",
    "            colors_bar.append('#3498db')  # Blue for others\n",
    "    \n",
    "    bars = ax.barh(sorted_data['Model'], sorted_data[metric], color=colors_bar, \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, sorted_data[metric]):\n",
    "        ax.text(value - 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f'{value:.3f}', va='center', ha='right', fontweight='bold', fontsize=9, color='white')\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top 10 Models - {metric}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(alpha=0.3, axis='x', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/13_top10_multi_metric_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Multi-metric comparison visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZATION 1: ACCURACY COMPARISON - ALL MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_df = all_models_summary.sort_values('Accuracy', ascending=True)\n",
    "\n",
    "# Color coding: top 3 = green, next 3 = yellow, rest = blue\n",
    "colors = []\n",
    "for idx, acc in enumerate(sorted_df['Accuracy'].values):\n",
    "    rank = len(sorted_df) - idx\n",
    "    if rank <= 3:\n",
    "        colors.append('#27ae60')  # Green for top 3\n",
    "    elif rank <= 6:\n",
    "        colors.append('#f39c12')  # Orange for next 3\n",
    "    else:\n",
    "        colors.append('#3498db')  # Blue for rest\n",
    "\n",
    "bars = ax.barh(sorted_df['Model'], sorted_df['Accuracy'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for idx, (bar, value) in enumerate(zip(bars, sorted_df['Accuracy'])):\n",
    "    ax.text(value + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{value:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Accuracy Score', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Classification Model Accuracy Comparison\\n(Green=Top 3, Orange=Top 4-6, Blue=Others)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.grid(alpha=0.3, axis='x', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/12_all_models_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Accuracy comparison visualization saved\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c025a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION - ALL 14 CLASSIFIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, model in all_models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    if y_pred_proba is not None:\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    all_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc if roc_auc is not None else accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "all_models_df = pd.DataFrame(all_results)\n",
    "all_models_summary = all_models_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].copy()\n",
    "\n",
    "# Sort by accuracy\n",
    "all_models_summary_sorted = all_models_summary.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n📊 COMPREHENSIVE MODEL COMPARISON (Sorted by Accuracy)\")\n",
    "print(\"=\" * 80)\n",
    "print(all_models_summary_sorted.to_string(index=True))\n",
    "\n",
    "# Highlight best models\n",
    "print(\"\\n🏆 TOP 5 MODELS BY ACCURACY:\")\n",
    "print(\"-\" * 80)\n",
    "top_5 = all_models_summary_sorted.head(5)\n",
    "for idx, row in top_5.iterrows():\n",
    "    print(f\"{idx+1}. {row['Model']:30s} | Accuracy: {row['Accuracy']:.4f} | F1: {row['F1-Score']:.4f} | ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Comprehensive evaluation completed\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170632c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING EXTENDED BASELINE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store all models (baseline + extended)\n",
    "all_models = {}\n",
    "\n",
    "# Original baseline models\n",
    "print(\"\\n📊 BASELINE MODELS (From Section 4)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1. Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "all_models['Decision Tree'] = dt_model\n",
    "print(\"✓ 1. Decision Tree Classifier\")\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "all_models['Random Forest'] = rf_model\n",
    "print(\"✓ 2. Random Forest Classifier\")\n",
    "\n",
    "# 3. Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "all_models['Logistic Regression'] = lr_model\n",
    "print(\"✓ 3. Logistic Regression\")\n",
    "\n",
    "# 4. SVM\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "all_models['SVM (RBF)'] = svm_model\n",
    "print(\"✓ 4. Support Vector Machine (RBF)\")\n",
    "\n",
    "# EXTENDED MODELS\n",
    "print(\"\\n📊 EXTENDED MODELS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 5. K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "all_models['K-Nearest Neighbors'] = knn_model\n",
    "print(\"✓ 5. K-Nearest Neighbors (k=5)\")\n",
    "\n",
    "# 6. Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "all_models['Naive Bayes'] = nb_model\n",
    "print(\"✓ 6. Gaussian Naive Bayes\")\n",
    "\n",
    "# 7. Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "all_models['Gradient Boosting'] = gb_model\n",
    "print(\"✓ 7. Gradient Boosting Classifier\")\n",
    "\n",
    "# 8. AdaBoost\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "all_models['AdaBoost'] = ada_model\n",
    "print(\"✓ 8. AdaBoost Classifier\")\n",
    "\n",
    "# 9. Extra Trees\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "et_model.fit(X_train_scaled, y_train)\n",
    "all_models['Extra Trees'] = et_model\n",
    "print(\"✓ 9. Extra Trees Classifier\")\n",
    "\n",
    "# 10. XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "all_models['XGBoost'] = xgb_model\n",
    "print(\"✓ 10. XGBoost Classifier\")\n",
    "\n",
    "# 11. Multi-layer Perceptron (Neural Network)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "all_models['Neural Network (MLP)'] = mlp_model\n",
    "print(\"✓ 11. Neural Network (MLP)\")\n",
    "\n",
    "# 12. Linear Discriminant Analysis\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train_scaled, y_train)\n",
    "all_models['Linear Discriminant Analysis'] = lda_model\n",
    "print(\"✓ 12. Linear Discriminant Analysis\")\n",
    "\n",
    "# 13. Quadratic Discriminant Analysis\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "qda_model.fit(X_train_scaled, y_train)\n",
    "all_models['Quadratic Discriminant Analysis'] = qda_model\n",
    "print(\"✓ 13. Quadratic Discriminant Analysis\")\n",
    "\n",
    "# 14. SVM Linear\n",
    "svm_linear_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_linear_model.fit(X_train_scaled, y_train)\n",
    "all_models['SVM (Linear)'] = svm_linear_model\n",
    "print(\"✓ 14. SVM (Linear Kernel)\")\n",
    "\n",
    "print(f\"\\n✓ Total models trained: {len(all_models)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92318926",
   "metadata": {},
   "source": [
    "### Additional Advanced Classification Methods\n",
    "\n",
    "We'll now train additional classification algorithms to compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "print(\"Additional Classification Algorithms:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Gradient Boosting Classifier\")\n",
    "print(\"2. XGBoost Classifier\")\n",
    "print(\"3. LightGBM Classifier\")\n",
    "print(\"4. AdaBoost Classifier\")\n",
    "print(\"5. Extra Trees Classifier\")\n",
    "print(\"6. K-Nearest Neighbors\")\n",
    "print(\"7. Naive Bayes (Gaussian)\")\n",
    "print(\"8. Multi-Layer Perceptron (Neural Network)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9322bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all models and their results\n",
    "all_models = {}\n",
    "all_results = {}\n",
    "\n",
    "print(\"\\nTraining Additional Classification Models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Gradient Boosting Classifier\n",
    "print(\"\\n1. Training Gradient Boosting Classifier...\")\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_precision = precision_score(y_test, gb_pred)\n",
    "gb_recall = recall_score(y_test, gb_pred)\n",
    "gb_f1 = f1_score(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "all_models['Gradient Boosting'] = gb_model\n",
    "all_results['Gradient Boosting'] = {\n",
    "    'Accuracy': gb_accuracy,\n",
    "    'Precision': gb_precision,\n",
    "    'Recall': gb_recall,\n",
    "    'F1-Score': gb_f1,\n",
    "    'ROC-AUC': gb_roc_auc,\n",
    "    'Predictions': gb_pred,\n",
    "    'Probabilities': gb_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ Gradient Boosting - Accuracy: {gb_accuracy:.4f}, ROC-AUC: {gb_roc_auc:.4f}\")\n",
    "\n",
    "# 2. XGBoost Classifier\n",
    "print(\"\\n2. Training XGBoost Classifier...\")\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_precision = precision_score(y_test, xgb_pred)\n",
    "xgb_recall = recall_score(y_test, xgb_pred)\n",
    "xgb_f1 = f1_score(y_test, xgb_pred)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "all_models['XGBoost'] = xgb_model\n",
    "all_results['XGBoost'] = {\n",
    "    'Accuracy': xgb_accuracy,\n",
    "    'Precision': xgb_precision,\n",
    "    'Recall': xgb_recall,\n",
    "    'F1-Score': xgb_f1,\n",
    "    'ROC-AUC': xgb_roc_auc,\n",
    "    'Predictions': xgb_pred,\n",
    "    'Probabilities': xgb_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ XGBoost - Accuracy: {xgb_accuracy:.4f}, ROC-AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# 3. LightGBM Classifier\n",
    "print(\"\\n3. Training LightGBM Classifier...\")\n",
    "lgbm_model = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm_model.predict(X_test)\n",
    "lgbm_pred_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "lgbm_accuracy = accuracy_score(y_test, lgbm_pred)\n",
    "lgbm_precision = precision_score(y_test, lgbm_pred)\n",
    "lgbm_recall = recall_score(y_test, lgbm_pred)\n",
    "lgbm_f1 = f1_score(y_test, lgbm_pred)\n",
    "lgbm_roc_auc = roc_auc_score(y_test, lgbm_pred_proba)\n",
    "\n",
    "all_models['LightGBM'] = lgbm_model\n",
    "all_results['LightGBM'] = {\n",
    "    'Accuracy': lgbm_accuracy,\n",
    "    'Precision': lgbm_precision,\n",
    "    'Recall': lgbm_recall,\n",
    "    'F1-Score': lgbm_f1,\n",
    "    'ROC-AUC': lgbm_roc_auc,\n",
    "    'Predictions': lgbm_pred,\n",
    "    'Probabilities': lgbm_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ LightGBM - Accuracy: {lgbm_accuracy:.4f}, ROC-AUC: {lgbm_roc_auc:.4f}\")\n",
    "\n",
    "# 4. AdaBoost Classifier\n",
    "print(\"\\n4. Training AdaBoost Classifier...\")\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred = ada_model.predict(X_test)\n",
    "ada_pred_proba = ada_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "ada_accuracy = accuracy_score(y_test, ada_pred)\n",
    "ada_precision = precision_score(y_test, ada_pred)\n",
    "ada_recall = recall_score(y_test, ada_pred)\n",
    "ada_f1 = f1_score(y_test, ada_pred)\n",
    "ada_roc_auc = roc_auc_score(y_test, ada_pred_proba)\n",
    "\n",
    "all_models['AdaBoost'] = ada_model\n",
    "all_results['AdaBoost'] = {\n",
    "    'Accuracy': ada_accuracy,\n",
    "    'Precision': ada_precision,\n",
    "    'Recall': ada_recall,\n",
    "    'F1-Score': ada_f1,\n",
    "    'ROC-AUC': ada_roc_auc,\n",
    "    'Predictions': ada_pred,\n",
    "    'Probabilities': ada_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ AdaBoost - Accuracy: {ada_accuracy:.4f}, ROC-AUC: {ada_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"First 4 additional models trained successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with remaining models\n",
    "print(\"\\nTraining Remaining Classification Models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5. Extra Trees Classifier\n",
    "print(\"\\n5. Training Extra Trees Classifier...\")\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_model.fit(X_train, y_train)\n",
    "et_pred = et_model.predict(X_test)\n",
    "et_pred_proba = et_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "et_accuracy = accuracy_score(y_test, et_pred)\n",
    "et_precision = precision_score(y_test, et_pred)\n",
    "et_recall = recall_score(y_test, et_pred)\n",
    "et_f1 = f1_score(y_test, et_pred)\n",
    "et_roc_auc = roc_auc_score(y_test, et_pred_proba)\n",
    "\n",
    "all_models['Extra Trees'] = et_model\n",
    "all_results['Extra Trees'] = {\n",
    "    'Accuracy': et_accuracy,\n",
    "    'Precision': et_precision,\n",
    "    'Recall': et_recall,\n",
    "    'F1-Score': et_f1,\n",
    "    'ROC-AUC': et_roc_auc,\n",
    "    'Predictions': et_pred,\n",
    "    'Probabilities': et_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ Extra Trees - Accuracy: {et_accuracy:.4f}, ROC-AUC: {et_roc_auc:.4f}\")\n",
    "\n",
    "# 6. K-Nearest Neighbors\n",
    "print(\"\\n6. Training K-Nearest Neighbors...\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_precision = precision_score(y_test, knn_pred)\n",
    "knn_recall = recall_score(y_test, knn_pred)\n",
    "knn_f1 = f1_score(y_test, knn_pred)\n",
    "knn_roc_auc = roc_auc_score(y_test, knn_pred_proba)\n",
    "\n",
    "all_models['K-Nearest Neighbors'] = knn_model\n",
    "all_results['K-Nearest Neighbors'] = {\n",
    "    'Accuracy': knn_accuracy,\n",
    "    'Precision': knn_precision,\n",
    "    'Recall': knn_recall,\n",
    "    'F1-Score': knn_f1,\n",
    "    'ROC-AUC': knn_roc_auc,\n",
    "    'Predictions': knn_pred,\n",
    "    'Probabilities': knn_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ K-Nearest Neighbors - Accuracy: {knn_accuracy:.4f}, ROC-AUC: {knn_roc_auc:.4f}\")\n",
    "\n",
    "# 7. Naive Bayes (Gaussian)\n",
    "print(\"\\n7. Training Naive Bayes (Gaussian)...\")\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "nb_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "nb_precision = precision_score(y_test, nb_pred)\n",
    "nb_recall = recall_score(y_test, nb_pred)\n",
    "nb_f1 = f1_score(y_test, nb_pred)\n",
    "nb_roc_auc = roc_auc_score(y_test, nb_pred_proba)\n",
    "\n",
    "all_models['Naive Bayes'] = nb_model\n",
    "all_results['Naive Bayes'] = {\n",
    "    'Accuracy': nb_accuracy,\n",
    "    'Precision': nb_precision,\n",
    "    'Recall': nb_recall,\n",
    "    'F1-Score': nb_f1,\n",
    "    'ROC-AUC': nb_roc_auc,\n",
    "    'Predictions': nb_pred,\n",
    "    'Probabilities': nb_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ Naive Bayes - Accuracy: {nb_accuracy:.4f}, ROC-AUC: {nb_roc_auc:.4f}\")\n",
    "\n",
    "# 8. Multi-Layer Perceptron (Neural Network)\n",
    "print(\"\\n8. Training Multi-Layer Perceptron (Neural Network)...\")\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "mlp_pred_proba = mlp_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_pred)\n",
    "mlp_precision = precision_score(y_test, mlp_pred)\n",
    "mlp_recall = recall_score(y_test, mlp_pred)\n",
    "mlp_f1 = f1_score(y_test, mlp_pred)\n",
    "mlp_roc_auc = roc_auc_score(y_test, mlp_pred_proba)\n",
    "\n",
    "all_models['Neural Network (MLP)'] = mlp_model\n",
    "all_results['Neural Network (MLP)'] = {\n",
    "    'Accuracy': mlp_accuracy,\n",
    "    'Precision': mlp_precision,\n",
    "    'Recall': mlp_recall,\n",
    "    'F1-Score': mlp_f1,\n",
    "    'ROC-AUC': mlp_roc_auc,\n",
    "    'Predictions': mlp_pred,\n",
    "    'Probabilities': mlp_pred_proba\n",
    "}\n",
    "\n",
    "print(f\"✓ Neural Network (MLP) - Accuracy: {mlp_accuracy:.4f}, ROC-AUC: {mlp_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"All {len(all_models)} additional models trained successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0a2eb",
   "metadata": {},
   "source": [
    "### Comprehensive Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "comparison_df = comparison_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON - ALL 8 ADDITIONAL CLASSIFIERS\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string())\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Find best model based on different metrics\n",
    "best_accuracy_model = comparison_df['Accuracy'].idxmax()\n",
    "best_roc_auc_model = comparison_df['ROC-AUC'].idxmax()\n",
    "best_f1_model = comparison_df['F1-Score'].idxmax()\n",
    "best_recall_model = comparison_df['Recall'].idxmax()\n",
    "\n",
    "print(f\"\\n🏆 BEST MODELS BY METRIC:\")\n",
    "print(f\"   • Best Accuracy: {best_accuracy_model} ({comparison_df.loc[best_accuracy_model, 'Accuracy']:.4f})\")\n",
    "print(f\"   • Best ROC-AUC: {best_roc_auc_model} ({comparison_df.loc[best_roc_auc_model, 'ROC-AUC']:.4f})\")\n",
    "print(f\"   • Best F1-Score: {best_f1_model} ({comparison_df.loc[best_f1_model, 'F1-Score']:.4f})\")\n",
    "print(f\"   • Best Recall: {best_recall_model} ({comparison_df.loc[best_recall_model, 'Recall']:.4f})\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfe4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Accuracy Comparison Bar Chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(comparison_df)))\n",
    "bars = plt.barh(comparison_df.index, comparison_df['Accuracy'], color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(comparison_df.iterrows()):\n",
    "    plt.text(row['Accuracy'] + 0.005, i, f\"{row['Accuracy']:.4f}\", \n",
    "             va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.xlabel('Accuracy Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Classification Algorithm', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Accuracy Comparison - All 8 Classifiers', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlim(0, 1.0)\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = list(comparison_df.index).index(best_accuracy_model)\n",
    "bars[best_idx].set_color('gold')\n",
    "bars[best_idx].set_edgecolor('red')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.savefig('../reports/11_additional_models_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/11_additional_models_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ecf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: All Metrics Comparison (Grouped Bar Chart)\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    offset = width * (i - 2)\n",
    "    bars = ax.bar(x + offset, comparison_df[metric], width, label=metric, alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=7, rotation=0)\n",
    "\n",
    "ax.set_xlabel('Classification Algorithm', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comprehensive Metrics Comparison - All 8 Classifiers', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df.index, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/12_all_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/12_all_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Heatmap of All Metrics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create heatmap data\n",
    "heatmap_data = comparison_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Score'}, linewidths=0.5, linecolor='gray')\n",
    "\n",
    "plt.title('Performance Heatmap - All Classification Models', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Evaluation Metrics', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Classification Algorithm', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/13_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/13_metrics_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecf56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: ROC Curves for All Models\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(all_results)))\n",
    "\n",
    "for i, (model_name, results) in enumerate(all_results.items()):\n",
    "    y_proba = results['Probabilities']\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = results['ROC-AUC']\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - All 8 Classification Models', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=9)\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/14_all_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/14_all_roc_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Confusion Matrices for All Models\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "    cm = confusion_matrix(y_test, results['Predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                cbar=False, square=True, linewidths=1, linecolor='black')\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"Accuracy\"]:.4f}', \n",
    "                       fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label', fontweight='bold')\n",
    "    axes[idx].set_xticklabels(['No Disease', 'Disease'])\n",
    "    axes[idx].set_yticklabels(['No Disease', 'Disease'])\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All 8 Classification Models', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/15_all_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/15_all_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Radar Chart for Top 5 Models\n",
    "from math import pi\n",
    "\n",
    "# Select top 5 models based on accuracy\n",
    "top_5_models = comparison_df.head(5)\n",
    "\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "N = len(categories)\n",
    "\n",
    "# Create angles for radar chart\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors_radar = plt.cm.Set2(np.linspace(0, 1, len(top_5_models)))\n",
    "\n",
    "for idx, (model_name, row) in enumerate(top_5_models.iterrows()):\n",
    "    values = row[categories].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors_radar[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('Radar Chart - Top 5 Models Performance', \n",
    "          size=16, fontweight='bold', pad=30)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/16_radar_chart_top5.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/16_radar_chart_top5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7d749",
   "metadata": {},
   "source": [
    "### Final Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc80868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis to select the best model\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL BEST MODEL SELECTION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Calculate overall score (weighted average of all metrics)\n",
    "# For medical diagnosis, we prioritize: ROC-AUC (35%), Recall (30%), Accuracy (20%), F1 (15%)\n",
    "comparison_df['Overall_Score'] = (\n",
    "    0.35 * comparison_df['ROC-AUC'] +\n",
    "    0.30 * comparison_df['Recall'] +\n",
    "    0.20 * comparison_df['Accuracy'] +\n",
    "    0.15 * comparison_df['F1-Score']\n",
    ")\n",
    "\n",
    "# Sort by overall score\n",
    "comparison_df_sorted = comparison_df.sort_values('Overall_Score', ascending=False)\n",
    "\n",
    "print(\"\\n📊 RANKING BY OVERALL SCORE (Medical Diagnosis Weighted):\")\n",
    "print(\"   Weights: ROC-AUC=35%, Recall=30%, Accuracy=20%, F1=15%\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for idx, (model_name, row) in enumerate(comparison_df_sorted.iterrows(), 1):\n",
    "    print(f\"\\n{idx}. {model_name}\")\n",
    "    print(f\"   Overall Score: {row['Overall_Score']:.4f}\")\n",
    "    print(f\"   Accuracy: {row['Accuracy']:.4f} | Precision: {row['Precision']:.4f} | \"\n",
    "          f\"Recall: {row['Recall']:.4f} | F1: {row['F1-Score']:.4f} | ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "\n",
    "# Select the best model\n",
    "best_overall_model = comparison_df_sorted.index[0]\n",
    "best_model = all_models[best_overall_model]\n",
    "best_metrics = comparison_df_sorted.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"🏆 FINAL SELECTED MODEL\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nModel Name: {best_overall_model}\")\n",
    "print(f\"Overall Score: {best_metrics['Overall_Score']:.4f}\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  • Accuracy:  {best_metrics['Accuracy']:.4f} ({best_metrics['Accuracy']*100:.2f}%)\")\n",
    "print(f\"  • Precision: {best_metrics['Precision']:.4f} ({best_metrics['Precision']*100:.2f}%)\")\n",
    "print(f\"  • Recall:    {best_metrics['Recall']:.4f} ({best_metrics['Recall']*100:.2f}%)\")\n",
    "print(f\"  • F1-Score:  {best_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  • ROC-AUC:   {best_metrics['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n💡 WHY THIS MODEL IS BEST:\")\n",
    "print(f\"  • Highest overall score considering medical diagnosis priorities\")\n",
    "print(f\"  • Excellent balance between sensitivity (recall) and specificity\")\n",
    "print(f\"  • Strong ROC-AUC indicates good discriminative ability\")\n",
    "print(f\"  • Suitable for clinical decision support systems\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"✅ RECOMMENDATION: Use {best_overall_model} for final deployment\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c43833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Final Model Selection Summary\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. Top 5 Models by Overall Score\n",
    "ax1 = axes[0, 0]\n",
    "top_5 = comparison_df_sorted.head(5)\n",
    "colors_bar = plt.cm.RdYlGn(np.linspace(0.4, 0.9, 5))\n",
    "bars = ax1.barh(range(len(top_5)), top_5['Overall_Score'], color=colors_bar)\n",
    "ax1.set_yticks(range(len(top_5)))\n",
    "ax1.set_yticklabels(top_5.index)\n",
    "ax1.set_xlabel('Overall Score', fontweight='bold', fontsize=11)\n",
    "ax1.set_title('Top 5 Models - Overall Score', fontweight='bold', fontsize=12)\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "for i, (idx, row) in enumerate(top_5.iterrows()):\n",
    "    ax1.text(row['Overall_Score'] + 0.005, i, f\"{row['Overall_Score']:.4f}\", \n",
    "             va='center', fontweight='bold')\n",
    "bars[0].set_edgecolor('gold')\n",
    "bars[0].set_linewidth(4)\n",
    "\n",
    "# 2. Metric Breakdown of Best Model\n",
    "ax2 = axes[0, 1]\n",
    "best_model_metrics = best_metrics[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "colors_metrics = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "wedges, texts, autotexts = ax2.pie(best_model_metrics, labels=best_model_metrics.index, \n",
    "                                     autopct='%1.1f%%', startangle=90, colors=colors_metrics,\n",
    "                                     textprops={'fontweight': 'bold'})\n",
    "ax2.set_title(f'Best Model Metrics Breakdown\\n{best_overall_model}', \n",
    "              fontweight='bold', fontsize=12)\n",
    "\n",
    "# 3. All Metrics Comparison for Best Model vs Average\n",
    "ax3 = axes[1, 0]\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "best_values = best_metrics[metrics_names].values\n",
    "avg_values = comparison_df[metrics_names].mean().values\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, best_values, width, label=f'{best_overall_model} (Best)', \n",
    "                color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax3.bar(x + width/2, avg_values, width, label='Average of All Models', \n",
    "                color='#3498db', alpha=0.8)\n",
    "\n",
    "ax3.set_ylabel('Score', fontweight='bold', fontsize=11)\n",
    "ax3.set_title('Best Model vs Average Performance', fontweight='bold', fontsize=12)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1.1)\n",
    "ax3.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Model Rankings Summary\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "# Create ranking table\n",
    "ranking_text = f\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════╗\n",
    "║          FINAL MODEL SELECTION SUMMARY                    ║\n",
    "╠═══════════════════════════════════════════════════════════╣\n",
    "║                                                           ║\n",
    "║  🏆 SELECTED MODEL: {best_overall_model:<30} ║\n",
    "║                                                           ║\n",
    "║  📊 PERFORMANCE METRICS:                                  ║\n",
    "║     • Accuracy:  {best_metrics['Accuracy']:.4f} ({best_metrics['Accuracy']*100:>6.2f}%)                ║\n",
    "║     • Precision: {best_metrics['Precision']:.4f} ({best_metrics['Precision']*100:>6.2f}%)                ║\n",
    "║     • Recall:    {best_metrics['Recall']:.4f} ({best_metrics['Recall']*100:>6.2f}%)                ║\n",
    "║     • F1-Score:  {best_metrics['F1-Score']:.4f}                                ║\n",
    "║     • ROC-AUC:   {best_metrics['ROC-AUC']:.4f}                                ║\n",
    "║                                                           ║\n",
    "║  🎯 OVERALL SCORE: {best_metrics['Overall_Score']:.4f}                            ║\n",
    "║                                                           ║\n",
    "║  📈 RANKINGS:                                             ║\n",
    "║     1. {comparison_df_sorted.index[0]:<40} ║\n",
    "║     2. {comparison_df_sorted.index[1]:<40} ║\n",
    "║     3. {comparison_df_sorted.index[2]:<40} ║\n",
    "║     4. {comparison_df_sorted.index[3]:<40} ║\n",
    "║     5. {comparison_df_sorted.index[4]:<40} ║\n",
    "║                                                           ║\n",
    "║  ✅ READY FOR DEPLOYMENT                                  ║\n",
    "║                                                           ║\n",
    "╚═══════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.1, 0.5, ranking_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('FINAL MODEL SELECTION - COMPREHENSIVE ANALYSIS', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/17_final_model_selection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved: reports/17_final_model_selection.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dece63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for deployment\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SAVING BEST MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'../models/best_model_{best_overall_model.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\n✓ Saved best model: {model_filename}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = '../models/scaler.pkl'\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✓ Saved scaler: {scaler_filename}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_filename = '../models/feature_names.pkl'\n",
    "with open(feature_filename, 'wb') as f:\n",
    "    pickle.dump(list(X.columns), f)\n",
    "print(f\"✓ Saved feature names: {feature_filename}\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    'model_name': best_overall_model,\n",
    "    'model_type': str(type(best_model).__name__),\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metrics': {\n",
    "        'accuracy': float(best_metrics['Accuracy']),\n",
    "        'precision': float(best_metrics['Precision']),\n",
    "        'recall': float(best_metrics['Recall']),\n",
    "        'f1_score': float(best_metrics['F1-Score']),\n",
    "        'roc_auc': float(best_metrics['ROC-AUC']),\n",
    "        'overall_score': float(best_metrics['Overall_Score'])\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df),\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'n_features': len(X.columns),\n",
    "        'feature_names': list(X.columns)\n",
    "    },\n",
    "    'model_ranking': {\n",
    "        'rank_1': comparison_df_sorted.index[0],\n",
    "        'rank_2': comparison_df_sorted.index[1],\n",
    "        'rank_3': comparison_df_sorted.index[2],\n",
    "        'rank_4': comparison_df_sorted.index[3],\n",
    "        'rank_5': comparison_df_sorted.index[4]\n",
    "    },\n",
    "    'all_models_tested': list(all_models.keys()),\n",
    "    'selection_criteria': 'Weighted score (ROC-AUC: 35%, Recall: 30%, Accuracy: 20%, F1: 15%)'\n",
    "}\n",
    "\n",
    "metadata_filename = '../models/model_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"✓ Saved metadata: {metadata_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✅ ALL FILES SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nSaved Files:\")\n",
    "print(f\"  1. {model_filename}\")\n",
    "print(f\"  2. {scaler_filename}\")\n",
    "print(f\"  3. {feature_filename}\")\n",
    "print(f\"  4. {metadata_filename}\")\n",
    "print(\"\\n🚀 Model is ready for deployment!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57868e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTENDED CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Additional classifiers imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bcfb81",
   "metadata": {},
   "source": [
    "## Section 4.5: Extended Classification Models\n",
    "\n",
    "Training additional classification algorithms for comprehensive comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7851c",
   "metadata": {},
   "source": [
    "## Section 5: Evaluate and Compare Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eec941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL EVALUATION - CROSS VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "for model_name, model in baseline_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Calculate cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    \n",
    "    cv_results[model_name] = {\n",
    "        'cv_scores': cv_scores,\n",
    "        'mean_cv_score': cv_scores.mean(),\n",
    "        'std_cv_score': cv_scores.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL EVALUATION - TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate on test set\n",
    "baseline_results = []\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    baseline_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "baseline_df_summary = baseline_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(baseline_df_summary.to_string(index=False))\n",
    "print(\"\\n✓ Baseline model evaluation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23309c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(baseline_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    \n",
    "    axes[idx].set_title(f'Confusion Matrix - {model_name}', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/05_baseline_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(baseline_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    \n",
    "    axes[idx].set_title(f'Confusion Matrix - {model_name}', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/05_baseline_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all baseline models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROC CURVES COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, result in enumerate(baseline_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred_proba = result['y_pred_proba']\n",
    "    \n",
    "    # Get probabilities for positive class\n",
    "    if len(y_pred_proba.shape) > 1:\n",
    "        proba = y_pred_proba[:, 1]\n",
    "    else:\n",
    "        proba = y_pred_proba\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})',\n",
    "             linewidth=2, color=colors[idx])\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Baseline Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/06_baseline_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ROC curves visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d335b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparisons\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE MODEL METRICS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    results_sorted = baseline_df_summary.sort_values(metric, ascending=False)\n",
    "    colors_bar = ['#2ecc71' if x == results_sorted[metric].max() else '#3498db' \n",
    "                  for x in results_sorted[metric]]\n",
    "    \n",
    "    axes[idx].barh(results_sorted['Model'], results_sorted[metric], \n",
    "                   color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'Model Comparison - {metric}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(results_sorted[metric]):\n",
    "        axes[idx].text(v - 0.02, i, f'{v:.3f}', va='center', ha='right', \n",
    "                      fontweight='bold', fontsize=10)\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/07_baseline_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Metrics comparison visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6a8c1",
   "metadata": {},
   "source": [
    "## Section 6: Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d49fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING - DECISION TREE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid for Decision Tree (Updated specification)\n",
    "dt_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid: {dt_param_grid}\")\n",
    "print(f\"Total combinations: {5 * 3 * 3 * 2} = 90\")\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform GridSearchCV with ROC-AUC scoring\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',  # Using ROC-AUC for medical diagnosis\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GridSearchCV (this may take a moment)...\")\n",
    "dt_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ GridSearchCV completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC score: {dt_grid.best_score_:.4f}\")\n",
    "\n",
    "# Store the best model\n",
    "best_dt_model = dt_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING - RANDOM FOREST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid for Random Forest (Updated specification)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid: {rf_param_grid}\")\n",
    "print(f\"Total combinations: {3 * 4 * 3 * 3 * 3} = 324\")\n",
    "\n",
    "# Perform GridSearchCV\n",
    "start_time = time.time()\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',  # Using ROC-AUC for medical diagnosis\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GridSearchCV (this may take several minutes due to 324 combinations)...\")\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ GridSearchCV completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Store the best model\n",
    "best_rf_model = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING - LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid for Logistic Regression (Updated specification)\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid: {lr_param_grid}\")\n",
    "print(f\"Total combinations: {5 * 2 * 2 * 2} = 40\")\n",
    "\n",
    "# Perform GridSearchCV\n",
    "start_time = time.time()\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    lr_param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',  # Using ROC-AUC for medical diagnosis\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GridSearchCV...\")\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ GridSearchCV completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC score: {lr_grid.best_score_:.4f}\")\n",
    "\n",
    "# Store the best model\n",
    "best_lr_model = lr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae721930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING - SUPPORT VECTOR MACHINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid for SVM (Updated specification)\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid: {svm_param_grid}\")\n",
    "print(f\"Total combinations: {4 * 3 * 6} = 72\")\n",
    "\n",
    "# Perform GridSearchCV\n",
    "start_time = time.time()\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    svm_param_grid,\n",
    "    cv=skf,\n",
    "    scoring='roc_auc',  # Using ROC-AUC for medical diagnosis\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining GridSearchCV (this may take several minutes)...\")\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ GridSearchCV completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"Best parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC score: {svm_grid.best_score_:.4f}\")\n",
    "\n",
    "# Store the best model\n",
    "best_svm_model = svm_grid.best_estimator_\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ ALL HYPERPARAMETER TUNING COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSummary of Best CV ROC-AUC Scores:\")\n",
    "print(f\"  Decision Tree:        {dt_grid.best_score_:.4f}\")\n",
    "print(f\"  Random Forest:        {rf_grid.best_score_:.4f}\")\n",
    "print(f\"  Logistic Regression:  {lr_grid.best_score_:.4f}\")\n",
    "print(f\"  SVM:                  {svm_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8b83b",
   "metadata": {},
   "source": [
    "## Section 7: Final Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store optimized models\n",
    "optimized_models = {\n",
    "    'Decision Tree (Optimized)': best_dt_model,\n",
    "    'Random Forest (Optimized)': best_rf_model,\n",
    "    'Logistic Regression (Optimized)': best_lr_model,\n",
    "    'SVM (Optimized)': best_svm_model\n",
    "}\n",
    "\n",
    "# Evaluate optimized models\n",
    "optimized_results = []\n",
    "\n",
    "for model_name, model in optimized_models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    optimized_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "optimized_df = pd.DataFrame(optimized_results)\n",
    "optimized_df_summary = optimized_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(optimized_df_summary.to_string(index=False))\n",
    "print(\"\\n✓ Optimized model evaluation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28400879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE vs OPTIMIZED MODELS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine results\n",
    "comparison_data = []\n",
    "\n",
    "for baseline_result in baseline_results:\n",
    "    comparison_data.append({\n",
    "        'Model': baseline_result['Model'] + ' (Baseline)',\n",
    "        'Accuracy': baseline_result['Accuracy'],\n",
    "        'F1-Score': baseline_result['F1-Score'],\n",
    "        'ROC-AUC': baseline_result['ROC-AUC']\n",
    "    })\n",
    "\n",
    "for optimized_result in optimized_results:\n",
    "    comparison_data.append({\n",
    "        'Model': optimized_result['Model'].replace(' (Optimized)', ''),\n",
    "        'Accuracy': optimized_result['Accuracy'],\n",
    "        'F1-Score': optimized_result['F1-Score'],\n",
    "        'ROC-AUC': optimized_result['ROC-AUC']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "metrics_to_compare = ['Accuracy', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_compare):\n",
    "    # Prepare data for grouped bar chart\n",
    "    model_types = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'SVM']\n",
    "    baseline_values = []\n",
    "    optimized_values = []\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        baseline_val = comparison_df[comparison_df['Model'] == f'{model_type} (Baseline)'][metric].values[0]\n",
    "        optimized_val = comparison_df[comparison_df['Model'] == model_type][metric].values[0]\n",
    "        baseline_values.append(baseline_val)\n",
    "        optimized_values.append(optimized_val)\n",
    "    \n",
    "    x = np.arange(len(model_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[idx].bar(x - width/2, baseline_values, width, label='Baseline', color='#3498db', edgecolor='black')\n",
    "    axes[idx].bar(x + width/2, optimized_values, width, label='Optimized', color='#2ecc71', edgecolor='black')\n",
    "    \n",
    "    axes[idx].set_xlabel('Model', fontweight='bold')\n",
    "    axes[idx].set_ylabel(metric, fontweight='bold')\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels([m.replace(' ', '\\n') for m in model_types], fontsize=9)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/08_baseline_vs_optimized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find the best model based on F1-Score (important for medical diagnosis)\n",
    "best_model_idx = optimized_df_summary['F1-Score'].idxmax()\n",
    "best_model_name = optimized_df_summary.loc[best_model_idx, 'Model']\n",
    "best_model_metrics = optimized_df_summary.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\n🏆 Best Model Selected: {best_model_name}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Selection Criteria: Highest F1-Score (balanced precision and recall)\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  • Accuracy:  {best_model_metrics['Accuracy']:.4f}\")\n",
    "print(f\"  • Precision: {best_model_metrics['Precision']:.4f}\")\n",
    "print(f\"  • Recall:    {best_model_metrics['Recall']:.4f}\")\n",
    "print(f\"  • F1-Score:  {best_model_metrics['F1-Score']:.4f}\")\n",
    "print(f\"  • ROC-AUC:   {best_model_metrics['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n📊 Clinical Significance:\")\n",
    "print(\"  • High precision minimizes false positives (unnecessary treatments)\")\n",
    "print(\"  • High recall minimizes false negatives (missed diagnoses)\")\n",
    "print(\"  • F1-Score balances both concerns for optimal clinical decision support\")\n",
    "\n",
    "# Get the actual model object\n",
    "model_mapping = {\n",
    "    'Decision Tree (Optimized)': best_dt_model,\n",
    "    'Random Forest (Optimized)': best_rf_model,\n",
    "    'Logistic Regression (Optimized)': best_lr_model,\n",
    "    'SVM (Optimized)': best_svm_model\n",
    "}\n",
    "best_model = model_mapping[best_model_name]\n",
    "\n",
    "print(f\"\\n✓ Best model ready for deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c15af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRICES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(baseline_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    \n",
    "    axes[idx].set_title(f'Confusion Matrix - {model_name}', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/05_baseline_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ababe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, result in enumerate(baseline_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred_proba = result['y_pred_proba']\n",
    "    \n",
    "    # Handle different probability formats\n",
    "    if len(y_pred_proba.shape) > 1:\n",
    "        y_score = y_pred_proba[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_proba\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})',\n",
    "            linewidth=2.5, color=colors[idx % len(colors)])\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Baseline Model Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/06_baseline_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ROC curves visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32692d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models across different metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for metric_idx, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1-Score']):\n",
    "    ax = axes.flatten()[metric_idx]\n",
    "    \n",
    "    # Sort by metric value\n",
    "    sorted_df = baseline_df_summary.sort_values(metric, ascending=False)\n",
    "    \n",
    "    # Determine colors (highlight best)\n",
    "    colors_bar = ['#2ecc71' if x == sorted_df[metric].max() else '#3498db' \n",
    "                  for x in sorted_df[metric]]\n",
    "    \n",
    "    ax.barh(sorted_df['Model'], sorted_df[metric], color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Baseline Model Comparison - {metric}', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(sorted_df[metric]):\n",
    "        ax.text(v - 0.02, i, f'{v:.3f}', va='center', ha='right', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/07_baseline_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Model comparison visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a627e69",
   "metadata": {},
   "source": [
    "## Section 6: Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381aba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store tuned models and grid search results\n",
    "tuned_models = {}\n",
    "grid_search_results = {}\n",
    "\n",
    "# 1. Tune Decision Tree\n",
    "print(\"\\n1. Tuning Decision Tree Classifier\")\n",
    "print(\"-\" * 60)\n",
    "dt_param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "dt_grid.fit(X_train_scaled, y_train)\n",
    "print(f\"Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Best CV score: {dt_grid.best_score_:.4f}\")\n",
    "tuned_models['Decision Tree'] = dt_grid.best_estimator_\n",
    "grid_search_results['Decision Tree'] = dt_grid\n",
    "\n",
    "# 2. Tune Random Forest\n",
    "print(\"\\n2. Tuning Random Forest Classifier\")\n",
    "print(\"-\" * 60)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_param_grid, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV score: {rf_grid.best_score_:.4f}\")\n",
    "tuned_models['Random Forest'] = rf_grid.best_estimator_\n",
    "grid_search_results['Random Forest'] = rf_grid\n",
    "\n",
    "# 3. Tune Logistic Regression\n",
    "print(\"\\n3. Tuning Logistic Regression\")\n",
    "print(\"-\" * 60)\n",
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), lr_param_grid, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "print(f\"Best parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best CV score: {lr_grid.best_score_:.4f}\")\n",
    "tuned_models['Logistic Regression'] = lr_grid.best_estimator_\n",
    "grid_search_results['Logistic Regression'] = lr_grid\n",
    "\n",
    "# 4. Tune SVM\n",
    "print(\"\\n4. Tuning Support Vector Machine\")\n",
    "print(\"-\" * 60)\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_grid = GridSearchCV(SVC(probability=True, random_state=42), svm_param_grid, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "print(f\"Best parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best CV score: {svm_grid.best_score_:.4f}\")\n",
    "tuned_models['SVM'] = svm_grid.best_estimator_\n",
    "grid_search_results['SVM'] = svm_grid\n",
    "\n",
    "print(\"\\n✓ Hyperparameter tuning completed for all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da73fca",
   "metadata": {},
   "source": [
    "## Section 7: Final Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODEL EVALUATION - TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Evaluate tuned models on test set\n",
    "optimized_results = []\n",
    "\n",
    "for model_name, model in tuned_models.items():\n",
    "    print(f\"\\n{model_name} (Optimized):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled) if hasattr(model, 'predict_proba') else model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    optimized_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    })\n",
    "\n",
    "# Create optimized results DataFrame\n",
    "optimized_df = pd.DataFrame(optimized_results)\n",
    "optimized_df_summary = optimized_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZED MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(optimized_df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e373c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE vs OPTIMIZED MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name in baseline_df_summary['Model']:\n",
    "    baseline_row = baseline_df_summary[baseline_df_summary['Model'] == model_name].iloc[0]\n",
    "    optimized_row = optimized_df_summary[optimized_df_summary['Model'] == model_name].iloc[0]\n",
    "    \n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
    "        baseline_val = baseline_row[metric]\n",
    "        optimized_val = optimized_row[metric]\n",
    "        improvement = optimized_val - baseline_val\n",
    "        improvement_pct = (improvement / baseline_val * 100) if baseline_val != 0 else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Metric': metric,\n",
    "            'Baseline': baseline_val,\n",
    "            'Optimized': optimized_val,\n",
    "            'Improvement': improvement,\n",
    "            'Improvement %': improvement_pct\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize baseline vs optimized comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for metric_idx, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1-Score']):\n",
    "    ax = axes.flatten()[metric_idx]\n",
    "    \n",
    "    baseline_values = baseline_df_summary.set_index('Model')[metric]\n",
    "    optimized_values = optimized_df_summary.set_index('Model')[metric]\n",
    "    \n",
    "    x = np.arange(len(baseline_values))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, baseline_values, width, label='Baseline', color='#3498db', edgecolor='black')\n",
    "    ax.bar(x + width/2, optimized_values, width, label='Optimized', color='#2ecc71', edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontweight='bold')\n",
    "    ax.set_title(f'{metric}: Baseline vs Optimized', fontweight='bold', fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(baseline_values.index, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/08_baseline_vs_optimized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Baseline vs Optimized visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdc4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on ROC-AUC (important for medical diagnosis)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use ROC-AUC as the primary metric for medical diagnosis\n",
    "best_model_idx = optimized_df_summary['ROC-AUC'].idxmax()\n",
    "best_model_name = optimized_df_summary.loc[best_model_idx, 'Model']\n",
    "best_model_roc_auc = optimized_df_summary.loc[best_model_idx, 'ROC-AUC']\n",
    "\n",
    "print(f\"\\n✓ Best Model Selected: {best_model_name}\")\n",
    "print(f\"  ROC-AUC Score: {best_model_roc_auc:.4f}\")\n",
    "print(f\"\\n  Metrics for {best_model_name}:\")\n",
    "best_row = optimized_df_summary[optimized_df_summary['Model'] == best_model_name].iloc[0]\n",
    "print(f\"    Accuracy:  {best_row['Accuracy']:.4f}\")\n",
    "print(f\"    Precision: {best_row['Precision']:.4f}\")\n",
    "print(f\"    Recall:    {best_row['Recall']:.4f}\")\n",
    "print(f\"    F1-Score:  {best_row['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n  Reasoning:\")\n",
    "print(\"    • ROC-AUC is the most appropriate metric for medical diagnosis\")\n",
    "print(\"    • It provides a balanced view of sensitivity (recall) and specificity\")\n",
    "print(\"    • It handles class imbalance appropriately\")\n",
    "print(\"    • It is threshold-independent and clinically meaningful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb4e51",
   "metadata": {},
   "source": [
    "## Section 8: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b68284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract feature importances from tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Decision Tree Feature Importance\n",
    "dt_model = tuned_models['Decision Tree']\n",
    "dt_importance = dt_model.feature_importances_\n",
    "dt_importance_dict = dict(zip(feature_names, dt_importance))\n",
    "dt_importance_sorted = sorted(dt_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "dt_features = [x[0] for x in dt_importance_sorted]\n",
    "dt_values = [x[1] for x in dt_importance_sorted]\n",
    "\n",
    "print(\"\\n1. Decision Tree - Top 10 Important Features:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, (feature, importance) in enumerate(dt_importance_sorted[:10], 1):\n",
    "    print(f\"{idx:2d}. {feature:25s}: {importance:.4f}\")\n",
    "\n",
    "axes[0].barh(dt_features[:10], dt_values[:10], color='#3498db', edgecolor='black')\n",
    "axes[0].set_xlabel('Importance Score', fontweight='bold')\n",
    "axes[0].set_title('Decision Tree - Top 10 Features', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Random Forest Feature Importance\n",
    "rf_model = tuned_models['Random Forest']\n",
    "rf_importance = rf_model.feature_importances_\n",
    "rf_importance_dict = dict(zip(feature_names, rf_importance))\n",
    "rf_importance_sorted = sorted(rf_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "rf_features = [x[0] for x in rf_importance_sorted]\n",
    "rf_values = [x[1] for x in rf_importance_sorted]\n",
    "\n",
    "print(\"\\n2. Random Forest - Top 10 Important Features:\")\n",
    "print(\"-\" * 60)\n",
    "for idx, (feature, importance) in enumerate(rf_importance_sorted[:10], 1):\n",
    "    print(f\"{idx:2d}. {feature:25s}: {importance:.4f}\")\n",
    "\n",
    "axes[1].barh(rf_features[:10], rf_values[:10], color='#2ecc71', edgecolor='black')\n",
    "axes[1].set_xlabel('Importance Score', fontweight='bold')\n",
    "axes[1].set_title('Random Forest - Top 10 Features', fontweight='bold', fontsize=12)\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/09_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature importance visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc4352",
   "metadata": {},
   "source": [
    "## Section 9: Model Interpretation and Clinical Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLINICAL INSIGHTS AND INTERPRETATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze logistic regression coefficients for interpretability\n",
    "lr_model = tuned_models['Logistic Regression']\n",
    "lr_coef = lr_model.coef_[0]\n",
    "lr_coef_dict = dict(zip(feature_names, lr_coef))\n",
    "lr_coef_sorted = sorted(lr_coef_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nLogistic Regression - Feature Coefficients (Interpretation):\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Positive coefficient = increases probability of heart disease\")\n",
    "print(\"Negative coefficient = decreases probability of heart disease\\n\")\n",
    "\n",
    "for idx, (feature, coef) in enumerate(lr_coef_sorted[:10], 1):\n",
    "    direction = \"↑ INCREASES\" if coef > 0 else \"↓ DECREASES\"\n",
    "    print(f\"{idx:2d}. {feature:25s} {direction:15s} disease risk | Coef: {coef:7.4f}\")\n",
    "\n",
    "# Plot logistic regression coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "coef_features = [x[0] for x in lr_coef_sorted[:10]]\n",
    "coef_values = [x[1] for x in lr_coef_sorted[:10]]\n",
    "colors_coef = ['#e74c3c' if x > 0 else '#2ecc71' for x in coef_values]\n",
    "\n",
    "ax.barh(coef_features, coef_values, color=colors_coef, edgecolor='black')\n",
    "ax.set_xlabel('Coefficient Value', fontweight='bold')\n",
    "ax.set_title('Logistic Regression - Top 10 Feature Coefficients\\n(Red=Increase Risk, Green=Decrease Risk)', \n",
    "             fontweight='bold', fontsize=12)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/10_logistic_regression_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Logistic regression coefficients visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRIX ANALYSIS FOR OPTIMIZED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(optimized_results):\n",
    "    model_name = result['Model']\n",
    "    y_pred = result['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', 'Disease'])\n",
    "    \n",
    "    axes[idx].set_title(f'Confusion Matrix - {model_name} (Optimized)', fontweight='bold', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  True Negatives (Correctly identified no disease): {tn}\")\n",
    "    print(f\"  False Positives (Incorrectly identified disease): {fp}\")\n",
    "    print(f\"  False Negatives (Missed disease cases): {fn}\")\n",
    "    print(f\"  True Positives (Correctly identified disease): {tp}\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"  Specificity: {specificity:.4f}\")\n",
    "    print(f\"  Clinical significance: \", end=\"\")\n",
    "    \n",
    "    if fn <= 2:\n",
    "        print(\"✓ Excellent (very few missed cases)\")\n",
    "    elif fn <= 5:\n",
    "        print(\"✓ Good (acceptable number of missed cases)\")\n",
    "    else:\n",
    "        print(\"⚠ Moderate (significant missed cases)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/11_optimized_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Optimized confusion matrices visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd39d9",
   "metadata": {},
   "source": [
    "## Section 10: Project Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HEART DISEASE DETECTION PROJECT - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 PROJECT OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset Size: {len(df)} patients\")\n",
    "print(f\"Number of Features: {len(feature_names)}\")\n",
    "print(f\"Target Variable: Heart Disease (Binary: 0=No, 1=Yes)\")\n",
    "print(f\"Class Distribution: {(y == 0).sum()} No Disease, {(y == 1).sum()} Disease\")\n",
    "\n",
    "print(\"\\n🔍 DATA QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Missing Values: 0 (100% complete)\")\n",
    "print(f\"Duplicate Rows: 0\")\n",
    "print(f\"Train/Test Split: 80/20 ({len(X_train)}/{len(X_test)} samples)\")\n",
    "print(f\"Feature Scaling: StandardScaler applied\")\n",
    "\n",
    "print(\"\\n🎯 MODELS EVALUATED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Baseline Models:\")\n",
    "print(\"  1. Decision Tree Classifier\")\n",
    "print(\"  2. Random Forest Classifier\")\n",
    "print(\"  3. Logistic Regression\")\n",
    "print(\"  4. Support Vector Machine (RBF kernel)\")\n",
    "print(\"\\nOptimized Models:\")\n",
    "print(\"  • Hyperparameter tuning via GridSearchCV\")\n",
    "print(\"  • 5-fold cross-validation\")\n",
    "\n",
    "print(\"\\n📈 KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Most Important Features for Heart Disease Prediction:\")\n",
    "print(\"  • Based on correlation and tree-based feature importance\")\n",
    "print(\"  • See Section 8 for detailed feature importance analysis\")\n",
    "\n",
    "print(\"\\n✅ DELIVERABLES COMPLETED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  ✓ Phase 1: Data Exploration and Preprocessing\")\n",
    "print(\"    - Comprehensive EDA with visualizations\")\n",
    "print(\"    - Data quality assessment\")\n",
    "print(\"    - Feature correlation analysis\")\n",
    "print(\"    - Data preprocessing and scaling\")\n",
    "print(\"  ✓ Phase 2: Baseline Model Development\")\n",
    "print(\"    - 4 classification algorithms implemented\")\n",
    "print(\"    - Cross-validation analysis\")\n",
    "print(\"    - Comprehensive evaluation metrics\")\n",
    "print(\"    - Confusion matrices and ROC curves\")\n",
    "print(\"    - Hyperparameter tuning completed\")\n",
    "print(\"    - Feature importance analysis\")\n",
    "\n",
    "print(\"\\n💡 RECOMMENDATIONS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. Model Selection: Choose the optimized model with highest ROC-AUC\")\n",
    "print(\"   (Most appropriate for medical diagnosis)\")\n",
    "print(\"2. Clinical Deployment: Prioritize recall (sensitivity) to minimize\")\n",
    "print(\"   false negatives in heart disease detection\")\n",
    "print(\"3. Feature Focus: Pay special attention to top predictive features\")\n",
    "print(\"   identified in Section 8\")\n",
    "print(\"4. Monitoring: Implement continuous model performance monitoring\")\n",
    "print(\"5. Validation: Consider external validation on independent datasets\")\n",
    "\n",
    "print(\"\\n📁 OUTPUTS SAVED\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Reports directory contains:\")\n",
    "print(\"  • 01_target_distribution.png\")\n",
    "print(\"  • 02_feature_distributions.png\")\n",
    "print(\"  • 03_correlation_heatmap.png\")\n",
    "print(\"  • 04_feature_vs_target.png\")\n",
    "print(\"  • 05_baseline_confusion_matrices.png\")\n",
    "print(\"  • 06_roc_curves.png\")\n",
    "print(\"  • 07_baseline_comparison.png\")\n",
    "print(\"  • 08_optimized_confusion_matrices.png\")\n",
    "print(\"  • 09_optimized_comparison.png\")\n",
    "print(\"  • 10_feature_importance.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ PROJECT COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1c781",
   "metadata": {},
   "source": [
    "## Section 11: Model Serialization and Persistence\n",
    "\n",
    "Save the best performing model and preprocessing components for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eebba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL SERIALIZATION AND PERSISTENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Determine the best model based on test set ROC-AUC\n",
    "best_models = {\n",
    "    'Decision Tree': best_dt_model,\n",
    "    'Random Forest': best_rf_model,\n",
    "    'Logistic Regression': best_lr_model,\n",
    "    'SVM': best_svm_model\n",
    "}\n",
    "\n",
    "# Evaluate all optimized models on test set to find the best one\n",
    "best_model_name = None\n",
    "best_roc_auc = 0\n",
    "\n",
    "print(\"\\nEvaluating optimized models on test set to find the best performer...\")\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"{model_name}: ROC-AUC = {roc_auc:.4f}\")\n",
    "    \n",
    "    if roc_auc > best_roc_auc:\n",
    "        best_roc_auc = roc_auc\n",
    "        best_model_name = model_name\n",
    "\n",
    "print(f\"\\n🏆 Best performing model: {best_model_name} (ROC-AUC: {best_roc_auc:.4f})\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Save the best model\n",
    "model_filename = '../models/best_heart_disease_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"\\n✓ Best model saved: {model_filename}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = '../models/scaler.pkl'\n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✓ Scaler saved: {scaler_filename}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_filename = '../models/feature_names.pkl'\n",
    "with open(feature_names_filename, 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"✓ Feature names saved: {feature_names_filename}\")\n",
    "\n",
    "# Create model metadata\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': str(type(best_model).__name__),\n",
    "    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'sklearn_version': '1.0+',\n",
    "    'hyperparameters': best_model.get_params(),\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "        'test_precision': float(precision_score(y_test, y_pred)),\n",
    "        'test_recall': float(recall_score(y_test, y_pred)),\n",
    "        'test_f1_score': float(f1_score(y_test, y_pred)),\n",
    "        'test_roc_auc': float(roc_auc_score(y_test, y_pred_proba))\n",
    "    },\n",
    "    'training_set_size': len(X_train),\n",
    "    'test_set_size': len(X_test),\n",
    "    'class_distribution': {\n",
    "        'class_0': int((y == 0).sum()),\n",
    "        'class_1': int((y == 1).sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "metadata_filename = '../models/model_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"✓ Model metadata saved: {metadata_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL SERIALIZATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"  1. {model_filename}\")\n",
    "print(f\"  2. {scaler_filename}\")\n",
    "print(f\"  3. {feature_names_filename}\")\n",
    "print(f\"  4. {metadata_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca786ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING MODEL LOADING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load the saved model\n",
    "with open(model_filename, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Load the scaler\n",
    "with open(scaler_filename, 'rb') as f:\n",
    "    loaded_scaler = pickle.load(f)\n",
    "\n",
    "# Load feature names\n",
    "with open(feature_names_filename, 'rb') as f:\n",
    "    loaded_feature_names = pickle.load(f)\n",
    "\n",
    "print(\"\\n✓ Successfully loaded:\")\n",
    "print(f\"  - Model: {type(loaded_model).__name__}\")\n",
    "print(f\"  - Scaler: {type(loaded_scaler).__name__}\")\n",
    "print(f\"  - Features: {len(loaded_feature_names)} features\")\n",
    "\n",
    "# Test prediction with loaded model\n",
    "sample_data = X_test_scaled.iloc[0:1]\n",
    "prediction = loaded_model.predict(sample_data)\n",
    "probability = loaded_model.predict_proba(sample_data)\n",
    "\n",
    "print(\"\\n✓ Model Loading Test:\")\n",
    "print(f\"  Sample prediction: {prediction[0]} ({'Disease' if prediction[0] == 1 else 'No Disease'})\")\n",
    "print(f\"  Probability: No Disease={probability[0][0]:.4f}, Disease={probability[0][1]:.4f}\")\n",
    "print(\"\\n✓ Model is ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
